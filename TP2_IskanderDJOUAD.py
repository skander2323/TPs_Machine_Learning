# -*- coding: utf-8 -*-
"""Bienvenue dans Colaboratory

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

import pandas as pd
df = pd.read_csv('/content/car.data')
 

df['unacc'].replace('unacc',0 ,inplace=True)
df['unacc'].replace('good', 3,inplace=True)
df['unacc'].replace('vgood', 4,inplace=True)
df['unacc'].replace('acc', 2,inplace=True)
target = df['unacc']
del df['unacc']

df['vhigh'].replace('vhigh',0 ,inplace=True)

df['vhigh'].replace('high',1 ,inplace=True)
df['vhigh'].replace('med', 2,inplace=True)
df['vhigh'].replace('low', 3,inplace=True)

df['vhigh.1'].replace('vhigh',0 ,inplace=True)
df['vhigh.1'].replace('med', 1,inplace=True)
df['vhigh.1'].replace('low', 3,inplace=True)
df['vhigh.1'].replace('high',4 ,inplace=True)


df['small'].replace('small',0 ,inplace=True)
df['small'].replace('med', 1,inplace=True)
df['small'].replace('big', 2,inplace=True)

df['low'].replace('low',0 ,inplace=True)
df['low'].replace('med', 1,inplace=True)
df['low'].replace('high', 2,inplace=True)





df['2'].replace('5more',5 ,inplace=True)
df['2'].replace('more', 1,inplace=True)
df['2'].replace('3more',3 ,inplace=True)
df['2'].replace('4more', 4,inplace=True)

df['2.1'].replace('5more',5 ,inplace=True)
df['2.1'].replace('more', 1,inplace=True)
df['2.1'].replace('3more',3 ,inplace=True)
df['2.1'].replace('4more', 4,inplace=True)







np.unique(df['small'])
df



"""il y'as 4  exemples de chaque classe : unacc , acc , good , vgood

Avant de construire le modèle, séparons le jeu de données en deux : 70% pour
l’apprentissage, 30% pour le test.
"""

X, y = df, target
X,y

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X.values,target,train_size=0.7,random_state=0)

from sklearn import tree
clf = tree.DecisionTreeClassifier()
clf.fit(X_train, y_train)

tree.plot_tree(clf, filled=True)
print(clf.score(X_train, y_train))

clf1 = tree.DecisionTreeClassifier(max_depth = 3)
clf1.fit(X_train, y_train)
tree.plot_tree(clf1, filled=True)
print(clf1.score(X_train, y_train))

clf2 = tree.DecisionTreeClassifier(min_samples_leaf = 20)
clf2.fit(X_train, y_train)
tree.plot_tree(clf2, filled=True)
print(clf2.score(X_train, y_train))

from sklearn import model_selection
X_train, X_test, y_train, y_test = model_selection.train_test_split(df, target,
    test_size=0.95, random_state=0)
from sklearn.model_selection import GridSearchCV
from sklearn import tree
pgrid = {"max_depth": [1, 2, 3, 4, 5, 6, 7],
      "min_samples_split": [2, 3, 5, 10, 15, 20]}
grid_search = GridSearchCV(tree.DecisionTreeClassifier(), param_grid=pgrid, cv=10)
grid_search.fit(X_train, y_train)

print(grid_search.best_estimator_)
print("le scoure est de "+str(grid_search.best_estimator_.score(X_test, y_test)))

df

import numpy as np
import matplotlib.pyplot as plt

# Paramètres
n_classes = 4
plot_colors = "bry" # blue-red-yellow
plot_step = 0.02

# Choisir les attributs longueur et largeur des pétales
pair = [1, 4]
data = df.values 
# On ne garde seulement les deux attributs
X = data [:,[1, 4]]
print(np.unique(X))
y = target
(print(y))
# Apprentissage de l'arbre
clf = tree.DecisionTreeClassifier().fit(X, y)
print(np.min(X[:, 0] - 1))
# Affichage de la surface de décision
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
#plt.xlabel(df.feature_names[pair[0]])
#plt.ylabel(df.feature_names[pair[1]])
plt.axis("tight")

# Affichage des points d'apprentissage
for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(y == i)
    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=y[i], cmap=plt.cm.Paired)
plt.axis("tight")
plt.suptitle("Decision surface of a decision tree using paired features")
plt.legend()
plt.show()